---
title: "Machine learning - write up"
output: html_document
---

Lets start by loading R packages.

```{r}
set.seed(1)
library(caret)
```

Next load training and testing data.

```{r}
training <- read.csv('pml-training.csv')
testing <- read.csv('pml-testing.csv')
```

Look at the training and tesing data. It is easy to see that in testing there is plenty of columns NA. Lets get rid of those.

```{r}
filterNA <-testing[,colSums(is.na(testing)) != nrow(testing)]
importantColumns <- names(filterNA)
importantColumns <- append(importantColumns, "classe")
trainingFiltered <- training[, names(training) %in% importantColumns]
trainingFiltered <- na.omit(trainingFiltered)
length(importantColumns)
```

Number of columns went down to 61. However there is still more than 19000 rows in training. Lets reduce it to something reasonable smaller subset and keep the rest.

```{r}
sample <- sample(1:nrow(trainingFiltered), 200, replace=FALSE)
trainingSample <- trainingFiltered[sample,]
trainingUsedForTesting <- trainingFiltered[-sample,]
```

Data are not really linear. Train with random forest.

```{r}
trainingFit <- train(classe ~ ., data = trainingSample, method = "rf", prox=TRUE)
trainingFit
```

On the training data it works with nice accuracy:

```{r}
trainingFit$results$Accuracy
```

Lets see how many from the training set were missplaced.

```{r}
pred <- predict(trainingFit, newdata = trainingUsedForTesting);
trainingUsedForTesting$predRight <- pred==trainingUsedForTesting$classe
table(pred,trainingUsedForTesting$classe)
```

It is easy to count how many missed out of total. And that is out of sample error: 201 / 19221 = 0.01045731.

```{r}
summary(trainingUsedForTesting$predRight)
```

Finally apply prediction model to testing data.

```{r}
predict(trainingFit, newdata=testing)
```

Strange that all belong to the same class. Perhaps on purpose.